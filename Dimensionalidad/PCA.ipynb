{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVZDIlgzmasX"
      },
      "source": [
        "### Reducción de dimensionalidad\n",
        "Supongamos que tenemos datos con muchas variables numéricas y queremos reducir su dimensionalidad mientras preservamos la mayor cantidad posible de la variabilidad original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW5t--IWmasc"
      },
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqgfSFbzmase"
      },
      "source": [
        "1. **Generación de datos simulados**:\n",
        "   - Creamos cuatro variables: tres de ellas (`x1`, `x2`, `x3`) están correlacionadas, mientras que la cuarta (`x4`) es independiente.\n",
        "   - Esto refleja un problema real donde algunas variables aportan información redundante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS-nC372mase"
      },
      "outputs": [],
      "source": [
        "# Paso 1: Crear datos simulados\n",
        "np.random.seed(42)\n",
        "n_samples = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiQyrTyUmasf"
      },
      "outputs": [],
      "source": [
        "# Variables altamente correlacionadas\n",
        "x1 = np.random.normal(50, 10, n_samples)\n",
        "x2 = x1 + np.random.normal(0, 5, n_samples)\n",
        "x3 = 2 * x1 + np.random.normal(0, 8, n_samples)\n",
        "x4 = np.random.normal(60, 15, n_samples)  # Variable independiente\n",
        "\n",
        "# Crear un DataFrame\n",
        "data = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXQQUU8masf",
        "outputId": "8a6f03d9-6df9-455b-9dcf-905e432b6f0f"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7W0Nnagmasg"
      },
      "source": [
        "2. **Estandarización**:\n",
        "   - Es importante estandarizar los datos antes de aplicar PCA, ya que esta técnica es sensible a las escalas de las variables.\n",
        "   - Utilizamos `StandardScaler` para transformar las variables para que tengan media 0 y desviación estándar 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bONrQsRemash"
      },
      "outputs": [],
      "source": [
        "# Paso 2: Estandarizar los datos\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05BDAfG5mash"
      },
      "source": [
        "3. **Aplicar PCA**:\n",
        "   - Usamos `PCA` de `sklearn` para descomponer la matriz de datos en componentes principales.\n",
        "   - Cada componente principal es una combinación lineal de las variables originales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC4DEEbDmasi",
        "outputId": "4300f476-f88a-4d41-9011-e999c1e19737"
      },
      "outputs": [],
      "source": [
        "# Paso 3: Aplicar PCA\n",
        "pca = PCA()\n",
        "pca.fit(data_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrK5-BUbmasi"
      },
      "outputs": [],
      "source": [
        "# Transformar los datos al nuevo espacio PCA\n",
        "data_pca = pca.transform(data_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "penV2x9zmasj"
      },
      "source": [
        "4. **Resultados del PCA**:\n",
        "   - `explained_variance_ratio`: Indica cuánta varianza explica cada componente.\n",
        "   - `components_`: Contiene los pesos o coeficientes que relacionan las variables originales con los componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlC_4PPcmasj"
      },
      "outputs": [],
      "source": [
        "# Paso 4: Análisis de los resultados\n",
        "# a) Proporción de varianza explicada por cada componente\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# b) Cargar los componentes principales\n",
        "components = pd.DataFrame(pca.components_, columns=data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfHJp7-Mmask"
      },
      "source": [
        "5. **Visualización**:\n",
        "   - Mostramos la proporción acumulada de varianza explicada para identificar cuántos componentes son suficientes para representar los datos.\n",
        "   - Generalmente, seleccionamos componentes que expliquen al menos el 85-90% de la varianza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPwL8_w4mask",
        "outputId": "3fae31e8-8353-4831-b116-613a8b57196e"
      },
      "outputs": [],
      "source": [
        "# Paso 5: Visualización de los resultados\n",
        "# Gráfico de varianza explicada acumulada\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(\n",
        "    range(1, len(explained_variance_ratio) + 1),\n",
        "    np.cumsum(explained_variance_ratio),\n",
        "    marker='o'\n",
        ")\n",
        "plt.title('Varianza Explicada Acumulada')\n",
        "plt.xlabel('Número de Componentes Principales')\n",
        "plt.ylabel('Proporción de Varianza Explicada')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnSzyxRgmasl"
      },
      "source": [
        "El gráfico de la varianza explicada acumulada muestra que **dos componentes principales son suficientes para explicar casi el 94% de la varianza total**. Esto implica que podemos reducir las dimensiones de los datos de cuatro a dos sin perder mucha información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0dGX3WKmasl"
      },
      "source": [
        "6. **Interpretación**:\n",
        "   - Imprimimos los pesos de cada variable en cada componente para entender qué variables contribuyen más.\n",
        "   - También transformamos los datos originales al nuevo espacio definido por los componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MELgJHCNmasl",
        "outputId": "8e42a0cd-fc32-454d-9305-f1fb14e4e219"
      },
      "outputs": [],
      "source": [
        "# Paso 6: Interpretación de los resultados\n",
        "print(\"Proporción de varianza explicada por cada componente:\")\n",
        "print(explained_variance_ratio)\n",
        "\n",
        "print(\"\\nCargar los componentes principales (matriz de pesos):\")\n",
        "print(components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GQ6Lc5smasl"
      },
      "source": [
        "\n",
        "- **PC1**: Las variables `x1`, `x2` y `x3` tienen pesos altos y similares (~0.57), lo que indica que este componente representa una combinación lineal de estas variables, que están altamente correlacionadas.\n",
        "- **PC2**: Tiene un peso dominante en `x4` (0.9946). Esto confirma que `x4` es una variable independiente y aporta información única.\n",
        "- **PC3**: Diferencia principalmente entre `x2` y `x3`, con pesos opuestos (0.738 y -0.665, respectivamente), lo que indica una relación inversa entre estas variables.\n",
        "- **PC4**: Tiene una fuerte asociación con `x1` (0.804), pero su varianza explicada es mínima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuZVprRPmasl",
        "outputId": "a54e06be-6b29-45ed-ecc8-42e33d83eacb"
      },
      "outputs": [],
      "source": [
        "# Crear un DataFrame con las nuevas variables\n",
        "data_pca_df = pd.DataFrame(data_pca, columns=[f'PC{i+1}' for i in range(len(data.columns))])\n",
        "\n",
        "print(\"\\nPrimeras 5 observaciones transformadas:\")\n",
        "print(data_pca_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhVLaAzxmasm"
      },
      "source": [
        "- Los valores ahora están en un nuevo sistema de coordenadas basado en los componentes principales.\n",
        "- Por ejemplo, la primera observación tiene una fuerte proyección en `PC1` (0.695) y una contribución moderada en `PC2` (0.652)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpzaiVr4masm"
      },
      "source": [
        "### **Conclusión**\n",
        "1. **Reducción de dimensionalidad**: Podemos reducir las cuatro variables originales (`x1`, `x2`, `x3`, `x4`) a dos componentes principales (`PC1`, `PC2`), manteniendo casi toda la información relevante.\n",
        "2. **Interpretación de componentes**:\n",
        "   - `PC1` representa la combinación de variables correlacionadas (`x1`, `x2`, `x3`).\n",
        "   - `PC2` captura la varianza de la variable independiente `x4`.\n",
        "3. **Utilidad práctica**: Esta reducción es útil para simplificar modelos predictivos, eliminar redundancia y visualizar los datos en un espacio bidimensional sin perder características importantes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
