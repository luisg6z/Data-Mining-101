{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e12005",
   "metadata": {},
   "source": [
    "# Actividad Práctica Experimental 9\n",
    "\n",
    "Esta actividad busca investigar acerca de la reducción de dimensionalidad y sus ventajas\n",
    "\n",
    "\n",
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1099530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06400625",
   "metadata": {},
   "source": [
    "## Carga del Dataset\n",
    "\n",
    "\n",
    "En este caso se usa el dataset iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0d1d8",
   "metadata": {},
   "source": [
    "## Estandarización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20420f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b974e9",
   "metadata": {},
   "source": [
    "## Reducción de Dimensionalidad usando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f10e7",
   "metadata": {},
   "source": [
    "### Varianza explicada para cada componente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae87d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(\"\\nVarianza explicada por cada componente:\")\n",
    "for i, (var, cum_var) in enumerate(zip(explained_variance, cumulative_variance)):\n",
    "    print(f\"Componente {i+1}: {var:.3f} ({cum_var:.3f} acumulada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4e7d9",
   "metadata": {},
   "source": [
    "### Reducción a 2 dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935f0be",
   "metadata": {},
   "source": [
    "### Visualización de la varianza y de las dimensiones reducidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    range(1, len(explained_variance) + 1),\n",
    "    cumulative_variance,\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Varianza Explicada Acumulada')\n",
    "plt.xlabel('Número de Componentes Principales')\n",
    "plt.ylabel('Proporción de Varianza Explicada')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(X_pca_2d[y == i, 0], X_pca_2d[y == i, 1], alpha=0.8, label=target_name)\n",
    "plt.xlabel('Primer componente principal')\n",
    "plt.ylabel('Segunda componente principal')\n",
    "plt.title('PCA - Iris dataset')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072afcf",
   "metadata": {},
   "source": [
    "### Análisis\n",
    "\n",
    "Los primeros dos componentes principales explican aproximadamente el 95.8% de la varianza (73.0% + 22.8%).\n",
    "\n",
    "En el gráfico de dispersión, se observa una buena separación entre la clase setosa y las otras dos clases. Las clases versicolor y virginica muestran cierta superposición, pero aún se pueden distinguir en gran medida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776bee8",
   "metadata": {},
   "source": [
    "## Usando t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ea906",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77881a8",
   "metadata": {},
   "source": [
    "## Comparación de ambos algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(X_pca_2d[y == i, 0], X_pca_2d[y == i, 1], alpha=0.8, label=target_name)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(X_tsne[y == i, 0], X_tsne[y == i, 1], alpha=0.8, label=target_name)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74917369",
   "metadata": {},
   "source": [
    "### Comparación\n",
    "\n",
    "1. t-SNE muestra una mejor separación entre las tres clases, especialmente entre versicolor y virginica que en PCA aparecían más superpuestas.\n",
    "\n",
    "2. t-SNE parece más útil para visualización ya que maximiza la separación entre clases, aunque PCA es más interpretable ya que los componentes principales tienen significado en términos de las variables originales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24762c",
   "metadata": {},
   "source": [
    "## Implementación de KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793d95c",
   "metadata": {},
   "source": [
    "### Usando el conjunto de datos original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ccc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "knn_original = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_original.fit(X_train, y_train)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "y_pred_original = knn_original.predict(X_test)\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2eafa",
   "metadata": {},
   "source": [
    "### Usando los datos con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1aeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca_2d.transform(scaler.transform(X_train))\n",
    "X_test_pca = pca_2d.transform(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "pca_time = time.time() - start_time\n",
    "\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dff89",
   "metadata": {},
   "source": [
    "## Comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComparación de resultados:\")\n",
    "print(f\"Datos originales - Exactitud: {accuracy_original:.4f}, Tiempo entrenamiento: {original_time:.6f} seg\")\n",
    "print(f\"PCA (2 componentes) - Exactitud: {accuracy_pca:.4f}, Tiempo entrenamiento: {pca_time:.6f} seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594aace",
   "metadata": {},
   "source": [
    "### Exactitud:\n",
    "\n",
    "El modelo entrenado con el conjunto de los datos originales obtuvo un 100% de exactitud, mientras que el modelo en el que se usó PCA alcanzó un ~95%. Esto se debe a que el modelo entrenado con los datos originales dispone de todas las características, sin embargo, el modelo que usó PCA alcanzó una precisión bastante buena.\n",
    "\n",
    "### Tiempo de entrenamiento:\n",
    "\n",
    "El entrenamiento usando PCA es más rápido, pero en este dataset no es muy notoria la diferencia debido a que el dataset no cuenta con muchas características y cuenta con pocos datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186d356",
   "metadata": {},
   "source": [
    "## Preguntas de Control:\n",
    "\n",
    "\n",
    "### 1. ¿Cómo cambia el rendimiento con reducción de dimensionalidad?\n",
    "\n",
    "\n",
    "El rendimiento puede mejorar con la reducción de dimensionalidad cuando se elimina ruido o características irrelevantes, lo que ayuda a reducir el sobreajuste y acelera el entrenamiento del modelo. Sin embargo, si se pierde información relevante durante el proceso, el rendimiento puede disminuir, especialmente en conjuntos de datos donde todas las características son importantes para la predicción.\n",
    "\n",
    "\n",
    "\n",
    "### 2. ¿En qué casos conviene aplicar PCA antes de entrenar un modelo?\n",
    "\n",
    "\n",
    "Aplicar PCA antes de entrenar un modelo es conveniente cuando se trabaja con datos de alta dimensionalidad y multicolinealidad, o cuando se necesita reducir el costo computacional sin perder demasiada información. También es útil para visualizar datos en 2D o 3D, o cuando se busca eliminar ruido y simplificar la estructura de los datos antes de modelar.\n",
    "\n",
    "\n",
    "\n",
    "### 3. ¿Qué ventajas tiene reducir dimensionalidad cuando se trata de modelos complejos o datos ruidosos?\n",
    "\n",
    "\n",
    "Reducir la dimensionalidad en modelos complejos o datos ruidosos ofrece ventajas como una menor susceptibilidad al sobreajuste, ya que se eliminan características redundantes o irrelevantes. Además, mejora la eficiencia computacional al trabajar con menos variables y puede aumentar la interpretabilidad del modelo. En datos ruidosos, técnicas como PCA ayudan a extraer las componentes principales más significativas, descartando variabilidad no informativa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
