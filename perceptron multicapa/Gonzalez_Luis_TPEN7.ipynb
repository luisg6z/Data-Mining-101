{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trabajo Práctico Experimental 7\n",
        "\n",
        "\n",
        "Este trabajo tiene como objetivo explorar el funcionamiento e implementación de redes neuronales usando backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importación de Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y4-3hysauJW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creación del Conjunto de Datos\n",
        "\n",
        "\n",
        "En este caso, se hace uso de la utilidad make_moons de SciKit-learn para crear un conjunto de datos de dos clases en forma de lunas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 400\n",
        "noise = 0.08\n",
        "random_state = 42 \n",
        "\n",
        "\n",
        "X, y = make_moons(\n",
        "    n_samples=n_samples,\n",
        "    noise=noise,\n",
        "    random_state=random_state\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70szk4aebBjp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Clase 0', alpha=0.6, edgecolors='w')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Clase 1', alpha=0.6, edgecolors='w')\n",
        "plt.title(\"Datos de Clasificación Binaria con 2 Clusters por Clase\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementación de la Red Neuronal con Backpropagation\n",
        "\n",
        "\n",
        "A continuación se muestra la implementación de la red neuronal, esta hace uso de varias funciones de activación (tanh, sigmoide, relu) y tiene una función softmax de salida, y este implementa un standard scaler al recibir datos para en entrenamiento y al hacer la predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk0Z0Cn1bKgx",
        "outputId": "3ab270cb-1878-4b71-edd6-792e50f3585f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, hidden_layers=(3, 6), activation='relu', learning_rate=0.01, seed=None):\n",
        "        \n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.seed = seed\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.scaler = StandardScaler()\n",
        "        \n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "    \n",
        "    def _initialize_parameters(self, input_dim, output_dim):\n",
        "        layer_dims = [input_dim] + list(self.hidden_layers) + [output_dim]\n",
        "        \n",
        "        for i in range(len(layer_dims) - 1):\n",
        "            if self.activation == 'relu':\n",
        "                scale = np.sqrt(2. / layer_dims[i])\n",
        "            else:\n",
        "                scale = np.sqrt(1. / layer_dims[i])\n",
        "                \n",
        "            self.weights.append(np.random.randn(layer_dims[i], layer_dims[i+1]) * scale)\n",
        "            self.biases.append(np.zeros((1, layer_dims[i+1])))\n",
        "    \n",
        "    # Funciones de activación\n",
        "    def _activation_fn(self, z, derivative=False):\n",
        "        if self.activation == 'sigmoid':\n",
        "            if derivative:\n",
        "                s = self._activation_fn(z)\n",
        "                return s * (1 - s)\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "        \n",
        "        elif self.activation == 'tanh':\n",
        "            if derivative:\n",
        "                return 1 - np.tanh(z)**2\n",
        "            return np.tanh(z)\n",
        "        \n",
        "        elif self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return (z > 0).astype(float)\n",
        "            return np.maximum(0, z)\n",
        "    \n",
        "    def _forward_propagation(self, X):\n",
        "        activations = [X]\n",
        "        z_values = []\n",
        "        \n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
        "            a = self._activation_fn(z) if i < len(self.weights) - 1 else self._softmax(z)\n",
        "            \n",
        "            z_values.append(z)\n",
        "            activations.append(a)\n",
        "        \n",
        "        return activations, z_values\n",
        "    \n",
        "    def _softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    \n",
        "    # Método para calcular la pérdida\n",
        "    def _compute_loss(self, y, y_hat):\n",
        "        m = y.shape[0]\n",
        "        log_likelihood = -np.log(y_hat[range(m), y])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "    \n",
        "    def _backward_propagation(self, X, y, activations, z_values):\n",
        "        m = X.shape[0]\n",
        "        gradients = {}\n",
        "        layers = len(self.weights)\n",
        "        \n",
        "        dz = activations[-1]\n",
        "        dz[range(m), y] -= 1\n",
        "        dz /= m\n",
        "        \n",
        "        for l in reversed(range(layers)):\n",
        "            gradients[f'dW{l}'] = np.dot(activations[l].T, dz)\n",
        "            gradients[f'db{l}'] = np.sum(dz, axis=0, keepdims=True)\n",
        "            \n",
        "            if l > 0:\n",
        "                dz = np.dot(dz, self.weights[l].T) * self._activation_fn(z_values[l-1], derivative=True)\n",
        "        \n",
        "        # Update parameters\n",
        "        for l in range(layers):\n",
        "            self.weights[l] -= self.learning_rate * gradients[f'dW{l}']\n",
        "            self.biases[l] -= self.learning_rate * gradients[f'db{l}']\n",
        "    \n",
        "    def fit(self, X, y, epochs=1000, verbose=100):\n",
        "        X = self.scaler.fit_transform(X)\n",
        "        y = y.astype(int)\n",
        "        \n",
        "        input_dim = X.shape[1]\n",
        "        output_dim = len(np.unique(y))\n",
        "        self._initialize_parameters(input_dim, output_dim)\n",
        "        \n",
        "        for epoch in range(1, epochs + 1):\n",
        "            activations, z_values = self._forward_propagation(X)\n",
        "            \n",
        "            loss = self._compute_loss(y, activations[-1])\n",
        "            \n",
        "            self._backward_propagation(X, y, activations, z_values)\n",
        "            \n",
        "            if verbose and epoch % verbose == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = self.scaler.transform(X)\n",
        "        activations, _ = self._forward_propagation(X)\n",
        "        return np.argmax(activations[-1], axis=1)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        X = self.scaler.transform(X)\n",
        "        activations, _ = self._forward_propagation(X)\n",
        "        return activations[-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## División del conjunto de datos de entrenamiento y de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creación del modelo y entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn = NeuralNetwork(hidden_layers=(24, 12, 6), activation='tanh' , learning_rate=0.01, seed=42)\n",
        "nn.fit(X_train, y_train, epochs=2000, verbose=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicciones = nn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = ConfusionMatrixDisplay(confusion_matrix(y_test, predicciones), display_labels=np.unique(y))\n",
        "cm.plot(cmap=plt.cm.Blues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_rep = classification_report(y_test, predicciones)\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "accuracy = accuracy_score(y_test, predicciones)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejemplo de un dato cerca de la frontera de decisión\n",
        "\n",
        "\n",
        "Una de las ventajas de usar una capa softmax de salida, es que se puede saber las probabilidades de la pertenecia de un dato a las clases existentes, en este caso se muestra la probabilidad de pertenencia de un dato cerca de la frontera de decisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ex = nn.predict_proba(np.array([[-0.1, 0.5]]))\n",
        "print(f\"\\nProbabilidades de clase para el ejemplo [-0.1, 0.5]: {ex}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se puede observar que una clase no es ampliamente dominante, lo cual nos puede indicar incertidumbre en la clasificación del dato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados de la clasififcación de los datos originales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = nn.predict(X)\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=preds, palette=\"Set1\", marker=\".\")\n",
        "plt.title(\"Resultados de la clasificación con la Red Neuronal\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
